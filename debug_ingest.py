import lancedb
import os
import re
from pathlib import Path
from dotenv import load_dotenv
from lancedb.pydantic import LanceModel, Vector
from lancedb.embeddings import get_registry
from pydantic import Field

# 1. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()
print(f"üìç Current Working Directory: {os.getcwd()}")

# 2. –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—É—Ç–∏ (–ñ–ï–°–¢–ö–û, —á—Ç–æ–±—ã –Ω–µ –æ—à–∏–±–∏—Ç—å—Å—è)
# –ü—Ä–µ–¥–ª–∞–≥–∞–µ–º, —á—Ç–æ data –ª–µ–∂–∏—Ç –≤ –∫–æ—Ä–Ω–µ –ø—Ä–æ–µ–∫—Ç–∞
ROOT_DIR = Path(os.getcwd())
DATA_PATH = ROOT_DIR / "data" / "raw"
DB_PATH = ROOT_DIR / "data" / "lancedb"

print(f"üìÇ Looking for Markdown files in: {DATA_PATH}")
print(f"üíæ Database will be saved to:    {DB_PATH}")

# 3. –ú–æ–¥–µ–ª—å –¥–∞–Ω–Ω—ã—Ö (–ö–∞–∫ —É —Ç–µ–±—è)
embedding_model = (
    get_registry().get("sentence-transformers").create(name="BAAI/bge-small-en-v1.5")
)


class Transcript(LanceModel):
    id: str
    filename: str
    title: str
    text: str = embedding_model.SourceField()
    vector: Vector(384) = embedding_model.VectorField()


# 4. –§—É–Ω–∫—Ü–∏—è –æ—á–∏—Å—Ç–∫–∏
def clean_data(text: str):
    parts = text.lstrip().split("\n", 1)
    if len(parts) < 2:
        return None, "Too short/No newline"

    title = parts[0].lstrip("#").strip()
    content = parts[1]

    clean_text = re.sub(r"\[\d{2}:\d{2}:\d{2}\]", "", content)
    clean_text = re.sub(r"~~.*?~~", "", clean_text, flags=re.DOTALL)
    clean_text = re.sub(r"\s+", " ", clean_text).strip()

    if not clean_text:
        return None, "Empty content after clean"

    return (title, clean_text), "OK"


# 5. –ì–ª–∞–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞
def run_ingestion():
    if not DATA_PATH.exists():
        print(f"‚ùå ERROR: Raw data folder does not exist: {DATA_PATH}")
        return

    # –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è
    db = lancedb.connect(uri=DB_PATH)

    files = list(DATA_PATH.glob("*.md"))
    print(f"üîé Found {len(files)} .md files.")

    docs = []

    for file in files:
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã
        if re.search(r"\(\d+\)\.md$", file.name):
            print(f"SKIP (Duplicate): {file.name}")
            continue

        try:
            raw_text = file.read_text(encoding="utf-8")
            parsed, status = clean_data(raw_text)

            if parsed is None:
                print(f"SKIP ({status}): {file.name}")
                continue

            title, clean_text = parsed
            print(f"‚úÖ OK: {file.name} (Title: {title[:30]}...)")

            docs.append(
                {
                    "id": file.stem,
                    "filename": file.stem,
                    "title": title,
                    "text": clean_text,
                }
            )

        except Exception as e:
            print(f"‚ùå ERROR reading {file.name}: {e}")

    # –ó–∞–ø–∏—Å—å
    if docs:
        print(f"üöÄ Inserting {len(docs)} records into DB...")
        try:
            db.create_table(
                "transcripts", data=docs, schema=Transcript, mode="overwrite"
            )
            print("üéâ SUCCESS! Data inserted.")

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ä–∞–∑—É –∂–µ
            tbl = db.open_table("transcripts")
            print(f"üëÄ Verification: Table now has {tbl.count_rows()} rows.")
        except Exception as e:
            print(f"‚ùå ERROR during insertion: {e}")
    else:
        print("‚ö†Ô∏è  WARNING: No documents collected. Database remains empty.")


if __name__ == "__main__":
    run_ingestion()
